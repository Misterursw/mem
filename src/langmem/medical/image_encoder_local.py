"""
æœ¬åœ°BiomedCLIPæ¨¡å‹ç®¡ç†
å¤„ç†æ¨¡å‹ä¸‹è½½ã€ç¼“å­˜å’Œå¾®è°ƒ
"""

import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import json
import shutil

import torch
import numpy as np
from PIL import Image
from transformers import AutoModel, AutoProcessor

logger = logging.getLogger(__name__)


class LocalBiomedCLIP:
    """æœ¬åœ°BiomedCLIPæ¨¡å‹ç®¡ç†å™¨"""

    def __init__(
        self,
        model_name: str = "/root/.cache/huggingface/hub/models--microsoft--BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
        cache_dir: Optional[str] = None,
        device: Optional[str] = None
    ):
        """åˆå§‹åŒ–æœ¬åœ°BiomedCLIPæ¨¡å‹

        Args:
            model_name: HuggingFaceæ¨¡å‹åç§°
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            device: è®¡ç®—è®¾å¤‡
        """
        self.model_name = model_name
        # ä½¿ç”¨transformersåº“èƒ½è¯†åˆ«çš„ç»Ÿä¸€ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir or os.path.expanduser("~/.cache/langmem/biomedclip")
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        self._model = None
        self._processor = None
        self._is_finetuned = False

        # åˆ›å»ºç¼“å­˜ç›®å½•
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

    def download_model(self, force_download: bool = False):
        """ä¸‹è½½BiomedCLIPæ¨¡å‹åˆ°æœ¬åœ°

        Args:
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
        """
        print(f"ğŸ“¥ æ£€æŸ¥å¹¶ä¸‹è½½BiomedCLIPæ¨¡å‹åˆ°: {self.cache_dir}")

        if force_download and Path(self.cache_dir).exists():
            print(f"ğŸ§¹ å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼Œæ­£åœ¨æ¸…ç©ºç¼“å­˜ç›®å½•: {self.cache_dir}")
            shutil.rmtree(self.cache_dir)
            Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

        try:
            # ç›´æ¥ä½¿ç”¨HuggingFaceçš„ä¸‹è½½å’Œç¼“å­˜æœºåˆ¶
            print("ğŸ”„ æ­£åœ¨ä¸‹è½½/åŠ è½½å¤„ç†å™¨...")
            AutoProcessor.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir,
            )

            print("ğŸ”„ æ­£åœ¨ä¸‹è½½/åŠ è½½æ¨¡å‹...")
            model = AutoModel.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir,
            )
            print("âœ… BiomedCLIPæ¨¡å‹ä¸‹è½½/éªŒè¯å®Œæˆï¼")
            
            # æ‰“å°æ¨¡å‹å¤§å°ä¿¡æ¯
            model_size_mb = self._get_model_size(model)
            print(f"ğŸ“Š æ¨¡å‹å¤§å°: {model_size_mb}MB")

        except Exception as e:
            logger.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise

    def _get_model_size(self, model) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å°(MB)"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        size_mb = (param_size + buffer_size) / 1024**2
        return round(size_mb, 2)

    def load_model(self):
        """åŠ è½½æœ¬åœ°BiomedCLIPæ¨¡å‹"""
        if self._model is not None:
            return

        print(f"ğŸ”„ åŠ è½½BiomedCLIPæ¨¡å‹åˆ° {self.device}...")

        try:
            # ç®€åŒ–åŠ è½½é€»è¾‘ï¼Œç›´æ¥ä»ç¼“å­˜ç›®å½•åŠ è½½
            self._processor = AutoProcessor.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir
            )

            self._model = AutoModel.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir
            ).to(self.device)

            self._model.eval()

            print("âœ… BiomedCLIPæ¨¡å‹åŠ è½½å®Œæˆï¼")

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å°è¯•å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å‹...")
            try:
                self.download_model(force_download=True)
                # å†æ¬¡å°è¯•åŠ è½½
                self.load_model()
            except Exception as download_error:
                logger.error(f"å¼ºåˆ¶é‡æ–°ä¸‹è½½å¤±è´¥: {download_error}")
                raise download_error

    def encode_image(self, image_path: str) -> np.ndarray:
        """ç¼–ç å•å¼ å›¾åƒ"""
        if self._model is None:
            self.load_model()

        try:
            image = Image.open(image_path).convert("RGB")
            inputs = self._processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self._model.get_image_features(**inputs)
                features = outputs.cpu().numpy().flatten()

            return features

        except Exception as e:
            logger.error(f"å›¾åƒç¼–ç å¤±è´¥ {image_path}: {e}")
            raise
            
    # --- å¾®è°ƒç›¸å…³å‡½æ•° (ä¿æŒä¸å˜) ---
    def prepare_for_finetuning(
        self,
        bone_age_dataset: Optional[Dict] = None,
        learning_rate: float = 1e-5,
        freeze_backbone: bool = True
    ):
        """å‡†å¤‡éª¨é¾„å¾®è°ƒ"""
        if self._model is None:
            self.load_model()
        print("ğŸ¯ å‡†å¤‡BiomedCLIPéª¨é¾„å¾®è°ƒ...")
        if freeze_backbone:
            for param in self._model.vision_model.parameters():
                param.requires_grad = False
            print("ğŸ”’ å·²å†»ç»“è§†è§‰ä¸»å¹²ç½‘ç»œ")
        self._add_bone_age_head()
        trainable_params = filter(lambda p: p.requires_grad, self._model.parameters())
        optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate)
        print(f"âœ… å¾®è°ƒå‡†å¤‡å®Œæˆï¼Œå­¦ä¹ ç‡: {learning_rate}")
        return optimizer

    def _add_bone_age_head(self):
        """æ·»åŠ éª¨é¾„å›å½’å¤´"""
        feature_dim = self._model.config.projection_dim
        self._model.bone_age_head = torch.nn.Sequential(
            torch.nn.Linear(feature_dim, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(256, 1)
        ).to(self.device)
        print("ğŸ”§ å·²æ·»åŠ éª¨é¾„å›å½’å¤´")

    def finetune_on_bone_age(
        self,
        train_data: list,
        epochs: int = 10,
        batch_size: int = 8
    ):
        """åœ¨éª¨é¾„æ•°æ®ä¸Šå¾®è°ƒ"""
        if not hasattr(self._model, 'bone_age_head'):
            optimizer = self.prepare_for_finetuning()
        else:
            trainable_params = filter(lambda p: p.requires_grad, self._model.parameters())
            optimizer = torch.optim.AdamW(trainable_params, lr=1e-5)
        
        print(f"ğŸ¯ å¼€å§‹éª¨é¾„å¾®è°ƒ: {len(train_data)}ä¸ªæ ·æœ¬, {epochs}è½®")
        criterion = torch.nn.MSELoss()
        self._model.train()

        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            for i in range(0, len(train_data), batch_size):
                batch = train_data[i:i+batch_size]
                images = []
                targets = []
                for image_path, bone_age in batch:
                    try:
                        images.append(Image.open(image_path).convert("RGB"))
                        targets.append(bone_age)
                    except Exception as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆå›¾åƒ {image_path}: {e}")
                        continue
                if not images: continue
                
                inputs = self._processor(images=images, return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                targets = torch.tensor(targets, dtype=torch.float32).to(self.device)
                
                optimizer.zero_grad()
                image_features = self._model.get_image_features(**inputs)
                predictions = self._model.bone_age_head(image_features).squeeze()
                loss = criterion(predictions, targets)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            avg_loss = total_loss / max(1, num_batches)
            print(f"Epoch {epoch+1}/{epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        self._is_finetuned = True
        print("âœ… éª¨é¾„å¾®è°ƒå®Œæˆï¼")
        self.save_finetuned_model()

    def save_finetuned_model(self):
        """ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹"""
        if not self._is_finetuned:
            print("âš ï¸ æ¨¡å‹å°šæœªå¾®è°ƒï¼Œè·³è¿‡ä¿å­˜")
            return
        
        save_path = Path(self.cache_dir) / "finetuned_bone_age"
        save_path.mkdir(exist_ok=True)
        torch.save({
            'model_state_dict': self._model.state_dict(),
            'is_finetuned': True,
            'model_name': self.model_name
        }, save_path / "model.pt")
        print(f"ğŸ’¾ å¾®è°ƒæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
        save_path = Path(self.cache_dir) / "finetuned_bone_age" / "model.pt"
        if not save_path.exists():
            print("âŒ æœªæ‰¾åˆ°å¾®è°ƒæ¨¡å‹")
            return False
        
        try:
            self.load_model()
            self._add_bone_age_head()
            checkpoint = torch.load(save_path, map_location=self.device)
            self._model.load_state_dict(checkpoint['model_state_dict'])
            self._is_finetuned = True
            print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return True
        except Exception as e:
            logger.error(f"å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def predict_bone_age(self, image_path: str) -> float:
        """é¢„æµ‹éª¨é¾„ï¼ˆéœ€è¦å¾®è°ƒåçš„æ¨¡å‹ï¼‰"""
        if not self._is_finetuned:
            raise ValueError("éœ€è¦å…ˆå¾®è°ƒæ¨¡å‹æ‰èƒ½é¢„æµ‹éª¨é¾„")
        
        image = Image.open(image_path).convert("RGB")
        inputs = self._processor(images=image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            image_features = self._model.get_image_features(**inputs)
            bone_age = self._model.bone_age_head(image_features).squeeze()
            return bone_age.item()

def setup_local_biomedclip():
    """è®¾ç½®æœ¬åœ°BiomedCLIPç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®æœ¬åœ°BiomedCLIPç¯å¢ƒ...")
    biomedclip = LocalBiomedCLIP()
    # ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¼šä¸‹è½½
    biomedclip.load_model()
    return biomedclip

if __name__ == "__main__":
    # æµ‹è¯•æœ¬åœ°BiomedCLIP
    biomedclip = setup_local_biomedclip()
    
    print("\nğŸ§ª æµ‹è¯•å›¾åƒç¼–ç ...")
    print("   (è¯·å–æ¶ˆä¸‹é¢ä»£ç çš„æ³¨é‡Šï¼Œå¹¶æä¾›ä¸€å¼ çœŸå®å›¾åƒçš„è·¯å¾„è¿›è¡Œæµ‹è¯•)")
    # try:
    #     # åˆ›å»ºä¸€ä¸ªå‡çš„ç©ºç™½å›¾åƒç”¨äºæµ‹è¯•
    #     dummy_image_path = "test_hand_xray.jpg"
    #     Image.new('RGB', (224, 224), color = 'red').save(dummy_image_path)
    #     features = biomedclip.encode_image(dummy_image_path)
    #     print(f"âœ… å›¾åƒç¼–ç æˆåŠŸ! ç‰¹å¾ç»´åº¦: {features.shape}")
    #     os.remove(dummy_image_path)
    # except Exception as e:
    #     print(f"âŒ å›¾åƒç¼–ç æµ‹è¯•å¤±è´¥: {e}")

    print("\nâœ… æœ¬åœ°BiomedCLIPè®¾ç½®å®Œæˆï¼")